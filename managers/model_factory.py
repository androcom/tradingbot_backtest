import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from core import config

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'

import shutil
import logging
import pandas as pd
import optuna
from datetime import datetime

from core import utils
from pipelines.trainer import PipelineTrainer
from optimizers.param_tuner import objective as param_objective
from optimizers.logic_tuner import objective as logic_objective, prepare_simulation_data

# ë¡œê¹… ì„¤ì •
sys.stdout.reconfigure(line_buffering=True)
utils.silence_noisy_loggers()
log_path = os.path.join(config.LOG_BASE_DIR, "factory_system.log")
logger = utils.get_logger("Factory", log_file=log_path)

class ModelFactory:
    def __init__(self, num_models=5):
        self.num_models = num_models
        self.report_path = os.path.join(config.LOG_BASE_DIR, "factory_report.csv")
        self.results = []

    def run_factory(self):
        logger.info(f"ğŸ­ Starting Model Factory: Producing {self.num_models} Models...")

        for i in range(1, self.num_models + 1):
            model_name = f"Candidate_{i:02d}_{datetime.now().strftime('%H%M')}"
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ”¨ Processing Model {i}/{self.num_models}: {model_name}")
            logger.info(f"{'='*60}")

            # -----------------------------------------------------
            # Step 1: íŒŒë¼ë¯¸í„° íŠœë‹ (Hyperparameter Tuning)
            # -----------------------------------------------------
            logger.info("   [Step 1] Optimizing Hyperparameters...")
            study_param = optuna.create_study(direction='maximize')
            # ì‹œê°„ ë‹¨ì¶•ì„ ìœ„í•´ Trial íšŸìˆ˜ ì¡°ì ˆ
            study_param.optimize(param_objective, n_trials=25) 
            
            best_reward_params = study_param.best_params
            logger.info(f"   âœ… Best Params Found: {best_reward_params}")

            # Config ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸ (íŒŒì¼ì€ ê±´ë“œë¦¬ì§€ ì•ŠìŒ)
            self._apply_params_to_config(best_reward_params)

            # -----------------------------------------------------
            # Step 2: ë©”ì¸ ëª¨ë¸ í•™ìŠµ (Main Training)
            # -----------------------------------------------------
            logger.info("   [Step 2] Training Main Model (This takes long)...")
            
            # ì„¸ì…˜ ìƒì„±
            session = config.SessionManager()
            paths = session.create()
            
            # í•™ìŠµ ì‹¤í–‰
            trainer = PipelineTrainer(paths)
            trainer.run_all()
            
            # -----------------------------------------------------
            # Step 3: ë¡œì§ ìµœì í™” (Logic Tuning)
            # -----------------------------------------------------
            logger.info("   [Step 3] Optimizing Trading Logic...")
            
            # ë°©ê¸ˆ í•™ìŠµí•œ ëª¨ë¸ ê²½ë¡œë¡œ config ì„ì‹œ ìˆ˜ì •í•˜ì—¬ ë¡œì§ íŠœë„ˆê°€ ì¸ì‹í•˜ê²Œ í•¨
            # (Logic TunerëŠ” ê°€ì¥ ìµœê·¼ ëª¨ë¸ì„ ê°€ì ¸ì˜¤ë¯€ë¡œ ë³„ë„ ì¡°ì¹˜ ë¶ˆí•„ìš”í•˜ë‚˜ ì•ˆì „ì¥ì¹˜)
            prepare_simulation_data() # ìºì‹œ ê°±ì‹ 
            
            study_logic = optuna.create_study(direction='maximize')
            study_logic.optimize(logic_objective, n_trials=2500)
            
            best_logic = study_logic.best_params
            best_balance = study_logic.best_value
            logger.info(f"   âœ… Best Logic: Balance ${best_balance:,.2f}")

            # -----------------------------------------------------
            # Step 4: ëª¨ë¸ ì €ì¥ ë° ê¸°ë¡
            # -----------------------------------------------------
            save_dir = os.path.join(config.MODEL_BASE_DIR, model_name)
            
            # í•™ìŠµëœ ëª¨ë¸ ì´ë™
            if os.path.exists(save_dir): shutil.rmtree(save_dir)
            shutil.copytree(paths['model'], save_dir)
            
            # ê²°ê³¼ ê¸°ë¡
            result = {
                "Model": model_name,
                "Final_Balance": best_balance,
                "Reward_Params": str(best_reward_params),
                "Logic_Params": str(best_logic),
                "Path": save_dir
            }
            self.results.append(result)
            self._save_report()
            
            logger.info(f"ğŸ‰ Model {model_name} Completed.")

    def _apply_params_to_config(self, params):
        """Optunaì—ì„œ ì°¾ì€ íŒŒë¼ë¯¸í„°ë¥¼ í˜„ì¬ ë©”ëª¨ë¦¬ì˜ configì— ì ìš©"""
        # REWARD_PARAMS ì—…ë°ì´íŠ¸
        if not hasattr(config, 'REWARD_PARAMS'):
            config.REWARD_PARAMS = {}
        
        # PPO Learning Rate ë“± ë³„ë„ í‚¤ê°€ ìˆë‹¤ë©´ ë¶„ê¸° ì²˜ë¦¬
        for key, value in params.items():
            if key == 'learning_rate':
                config.RL_PPO_PARAMS['learning_rate'] = value
            elif key in ['profit_scale', 'teacher_bonus', 'teacher_penalty', 'mdd_penalty_factor', 'new_high_bonus']:
                config.REWARD_PARAMS[key] = value

    def _save_report(self):
        df = pd.DataFrame(self.results)
        df.to_csv(self.report_path, index=False)
        logger.info(f"ğŸ“„ Report updated: {self.report_path}")

if __name__ == "__main__":
    # ëª‡ ê°œì˜ ìƒ˜í”Œ ëª¨ë¸ì„ ë§Œë“¤ì§€ ì„¤ì • (ì˜ˆ: 3ê°œ)
    # 1ê°œë‹¹ ì•½ 24~30ì‹œê°„ ì†Œìš”ë˜ë¯€ë¡œ, ì£¼ë§ ë‚´ë‚´ ëŒë¦¬ë ¤ë©´ 2~3ê°œ ì¶”ì²œ
    factory = ModelFactory(num_models=3)
    factory.run_factory()