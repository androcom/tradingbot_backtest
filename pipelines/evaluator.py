import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from core import config

import logging
import joblib # [ì¶”ê°€] Scaler ë¡œë“œìš©
import pandas as pd
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from sklearn.preprocessing import RobustScaler

from core.data_loader import DataLoader
from core.trading_core import TradingCore
from core.rl_env import CryptoEnv
from models.hybrid_models import HybridLearner

class ModelEvaluator:
    def __init__(self):
        self.logger = logging.getLogger("Evaluator")
        self.loader = DataLoader(self.logger)

    def load_validation_data(self):
        """ìµœê·¼ ë°ì´í„° ë¡œë“œ (ê²€ì¦ìš©, Test Split ì´í›„)"""
        full_df = self.loader.get_ml_data(config.MAIN_SYMBOL)
        # í…ŒìŠ¤íŠ¸ ìŠ¤í”Œë¦¿ ì´í›„ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©
        val_df = full_df[full_df.index >= config.TEST_SPLIT_DATE].copy()
        return val_df

    def evaluate_model(self, model_dir, df):
        """íŠ¹ì • ëª¨ë¸ì˜ ìˆ˜ìµë¥  ê³„ì‚°"""
        try:
            # -----------------------------------------------------
            # [ìˆ˜ì •] Scaler ë¡œë“œ (Data Leakage ë°©ì§€)
            # -----------------------------------------------------
            feat_cols = [c for c in df.columns if c not in config.EXCLUDE_COLS]
            
            scaler_path = os.path.join(model_dir, "scaler.pkl")
            if os.path.exists(scaler_path):
                scaler = joblib.load(scaler_path)
                data_scaled = scaler.transform(df[feat_cols])
            else:
                self.logger.warning(f"âš ï¸ Scaler not found in {model_dir}. Fitting new scaler (Leakage Risk).")
                scaler = RobustScaler()
                data_scaled = scaler.fit_transform(df[feat_cols])
            
            # -----------------------------------------------------
            # ML Signal ìƒì„±
            # -----------------------------------------------------
            ml_model = HybridLearner(model_dir)
            
            # ì‹œí€€ìŠ¤ ìƒì„±
            X_seq = np.lib.stride_tricks.sliding_window_view(data_scaled, window_shape=(config.ML_SEQ_LEN, len(feat_cols)))
            # ì°¨ì› ì¶•ì†Œ (N, 1, Seq, Feat) -> (N, Seq, Feat)
            if X_seq.ndim == 4:
                X_seq = X_seq.squeeze(axis=1)
                
            X_flat = data_scaled[config.ML_SEQ_LEN:]
            
            # ê¸¸ì´ ë§ì¶¤
            min_len = min(len(X_seq), len(X_flat))
            
            # ì˜ˆì¸¡
            if not ml_model.load():
                self.logger.error("âŒ Failed to load ML models.")
                return -999, 0
                
            signals = ml_model.predict_proba(X_flat[:min_len], X_seq[:min_len])
            
            # DF ìŠ¬ë¼ì´ì‹± (ì‹œí€€ìŠ¤ ê¸¸ì´ë§Œí¼ ì•ë¶€ë¶„ ì œì™¸)
            sim_df = df.iloc[config.ML_SEQ_LEN:].iloc[:min_len].copy()
            sim_df['ml_signal'] = signals

            # -----------------------------------------------------
            # RL Backtest
            # -----------------------------------------------------
            env = CryptoEnv(sim_df, TradingCore(), precision_df=None, debug=False)
            env = DummyVecEnv([lambda: env])
            
            # VecNormalize ë¡œë“œ
            norm_path = os.path.join(model_dir, "vec_normalize.pkl")
            if os.path.exists(norm_path):
                env = VecNormalize.load(norm_path, env)
                env.training = False
                env.norm_reward = False

            agent_path = os.path.join(model_dir, "final_agent")
            model = PPO.load(agent_path)
            
            obs = env.reset()
            done = [False]
            info = {}
            
            while not done[0]:
                action, _ = model.predict(obs, deterministic=True)
                obs, _, done, info = env.step(action)
            
            # infoê°€ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ë¨ (Vectorized Env íŠ¹ì„±)
            final_balance = info[0]['final_balance']
            roi = (final_balance - config.INITIAL_BALANCE) / config.INITIAL_BALANCE * 100
            
            return roi, final_balance

        except Exception as e:
            self.logger.error(f"Evaluation Failed for {model_dir}: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return -999, 0

    def battle(self, champion_dir, challenger_dir):
        """ì±”í”¼ì–¸ vs ë„ì „ì ëŒ€ê²°"""
        df = self.load_validation_data()
        
        self.logger.info(f"âš”ï¸ BATTLE START: Champion vs Challenger")
        
        champ_roi, champ_bal = self.evaluate_model(champion_dir, df)
        self.logger.info(f"   ğŸ† Champion ROI: {champ_roi:.2f}% (${champ_bal:,.2f})")
        
        chall_roi, chall_bal = self.evaluate_model(challenger_dir, df)
        self.logger.info(f"   ğŸ¥Š Challenger ROI: {chall_roi:.2f}% (${chall_bal:,.2f})")
        
        # ë„ì „ìê°€ 5% ì´ìƒ ë” ì¢‹ì„ ë•Œë§Œ ìŠ¹ë¦¬ (êµì²´ ë¹„ìš© ê³ ë ¤)
        if chall_roi > champ_roi * 1.05:
            self.logger.info("ğŸ‰ NEW CHAMPION! Challenger Wins.")
            return "challenger"
        else:
            self.logger.info("ğŸ›¡ï¸ DEFENSE! Champion Remains.")
            return "champion"